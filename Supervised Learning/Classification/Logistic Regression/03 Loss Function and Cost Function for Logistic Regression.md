# Loss Function and Cost Function for Logistic Regression

---

## Goal of Logistic Regression

Model ka main kaam:
Input features (`X`) â†’ Linear Equation ($z = W \cdot X + b$) â†’ Sigmoid â†’ Output probability (`Å·`).

* **Å· (y-hat)**: Probability ki true label `y = 1` hai.

**Training ka goal**:
Best **W (weights)** aur **b (bias)** find karna jo `Å·` ko as close as possible kare `y` ke. Ye training process ko guide karte hain **Loss** aur **Cost Functions**.

---

## Loss Function â€“ Error on a Single Example

**Definition**: Ek single data point ke liye prediction kitni galat hai, uska measure.

### Why not use Linear Regression loss?
Linear Regression me simple error = $(y - \hat{y})^2$.
Logistic Regression me agar ye use karenge â†’ graph me multiple dips (local minima) ban jaayenge. **Gradient Descent** confuse ho jaayega aur galat jagah atak sakta hai.



### Solution: Binary Cross-Entropy (Log Loss)
Ye ek **convex** (bowl-shaped) function banata hai â†’ optimization easy ho jaata hai.

**Formula**:
$$L(\hat{y}, y) = -\Big[y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y})\Big]$$

### Intuition: Two Cases

#### Case 1: True label `y = 1`
Formula simplifies to:
$L = -\log(\hat{y})$
* Agar prediction `Å·` â†’ 1 (correct) â†’ $\log(\hat{y})$ â†’ 0 â†’ **Loss â‰ˆ 0** (Perfect prediction ðŸ‘).
* Agar prediction `Å·` â†’ 0 (wrong) â†’ $\log(\hat{y})$ â†’ -âˆž â†’ **Loss â‰« large** (Model punished ðŸ‘Ž).

Model ko reward milta hai jab `Å· â‰ˆ 1` aur penalty jab galat aur confident hai.

#### Case 2: True label `y = 0`
Formula simplifies to:
$L = -\log(1 - \hat{y})$
* Agar prediction `Å·` â†’ 0 (correct) â†’ $(1 - \hat{y}) \approx 1$ â†’ $\log(1) = 0$ â†’ **Loss â‰ˆ 0**.
* Agar prediction `Å·` â†’ 1 (wrong) â†’ $(1 - \hat{y}) \approx 0$ â†’ $\log(0) = -\infty$ â†’ **Loss â‰« large**.

Model ko reward milta hai jab `Å· â‰ˆ 0` aur penalty jab confidently galat hai.

---

## Loss vs Cost Function

* **Loss Function**: Ek single example ka error.
* **Cost Function**: Puri dataset ka average error.

> Matlab test dene par â†’ ek question ka score = **Loss**, aur poore test ka average = **Cost**.

### Cost Function Formula
$$J(W, b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)})$$
* **J(W, b)**: Cost function, jo weights aur bias pe depend karta hai.
* **m**: Total training examples.
* **Î£ (summation)**: Sab data points ka loss add karke average nikalna.
* **L(...)**: Loss function (binary cross-entropy) har ek example ke liye.

---

## Final Goal of Training

**Gradient Descent** use karke `W` aur `b` aise update karna ki `J(W, b)` minimize ho jaye. Jab Cost Function lowest ho â†’ model ke predictions sabse accurate hote hain poori dataset pe.

## Key Takeaways (One-liners for Revision)
* **Loss** = ek data point ka error.
* **Cost** = sabhi data points ka average error.
* Logistic Regression ke liye â†’ **Binary Cross-Entropy** loss best hai.
* Galti zyada penalty â†’ confidently wrong prediction.
* Optimization ke liye **Convex function** (bowl shape) hona zaroori hai.